{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5b77f7",
   "metadata": {},
   "source": [
    "# Data Analysis Project: Customer Churn Prediction\n",
    "\n",
    "This notebook outlines a simple machine learning pipeline to predict customer churn based on a hypothetical dataset. We'll cover data loading, preprocessing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "\n",
    "try:\n",
    "    # Simulate loading data\n",
    "    data = pd.DataFrame({\n",
    "        'CustomerID': range(1, 101),\n",
    "        'Age': np.random.randint(18, 65, 100),\n",
    "        'MonthlyCharges': np.random.uniform(20, 100, 100).round(2),\n",
    "        'TotalCharges': np.random.uniform(100, 5000, 100).round(2),\n",
    "        'Tenure': np.random.randint(1, 72, 100),\n",
    "        'Churn': np.random.choice([0, 1], 100, p=[0.7, 0.3])\n",
    "    })\n",
    "    print('Dummy data created.')\n",
    "    print(data.head())\n",
    "    print(data.info())\n",
    "except Exception as e:\n",
    "    print(f'Error loading data: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b51e1d",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "\n",
    "We'll handle missing values, encode categorical features, and scale numerical features to prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ffc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numerical_features = ['Age', 'MonthlyCharges', 'TotalCharges', 'Tenure']\n",
    "categorical_features = [] # No explicit categorical features in dummy data for now\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "X = data.drop('Churn', axis=1)._get_numeric_data() # Simplified for dummy data\n",
    "y = data['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print('Data preprocessed and split into training and testing sets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0be6e",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "A RandomForestClassifier will be trained on the preprocessed data, and its performance will be evaluated using accuracy and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b34149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_processed)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Model Accuracy: {accuracy:.4f}')\n",
    "print('\\nClassification Report:\\n', report)\n",
    "print('\\nConfusion Matrix:\\n', conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7249be5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully demonstrated a basic churn prediction pipeline. Further improvements could include more advanced feature engineering, hyperparameter tuning, and exploring different model architectures."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
